n = 10 sd = [ 0.05 , 0.05 ] x00 = np.random.normal ( loc= [ 0 , 0 ] , scale=sd , size= ( n,2 ) ) x01 = np.random.normal ( loc= [ 0 , 1 ] , scale=sd , size= ( n,2 ) ) x10 = np.random.normal ( loc= [ 1 , 0 ] , scale=sd , size= ( n,2 ) ) x11 = np.random.normal ( loc= [ 1 , 1 ] , scale=sd , size= ( n,2 ) ) x = np.vstack ( ( x00 , x01 , x10 , x11 ) ) y = np.vstack ( ( np.zeros ( ( x00.shape [ 0 ] ,1 ) ) , np.ones ( ( x01.shape [ 0 ] ,1 ) ) , np.ones ( ( x10.shape [ 0 ] ,1 ) ) , np.zeros ( ( x11.shape [ 0 ] ,1 ) ) ) ) .ravel ( ) ind = np.arange ( len ( y ) ) np.random.shuffle ( ind ) x = x [ ind ] y = y [ ind ] N = len ( y ) plt.scatter ( * x00.T , label='00 ' ) plt.scatter ( * x01.T , label='01 ' ) plt.scatter ( * x10.T , label='10 ' ) plt.scatter ( * x11.T , label='11 ' ) plt.legend ( ) plt.show ( ) Activation function : k = 10 def f ( s ) : return np.exp ( -k * ( s-1 ) * * 2 ) Initialize the weights , and train the network : w = np.random.uniform ( low=0.25 , high=1.75 , size= ( 2 ) ) print ( `` Initial w : '' , w ) rate = 0.01 n_epochs = 20 error = [ ] for _ in range ( n_epochs ) : err = 0 for i in range ( N ) : s = np.dot ( x [ i ] , w ) w -= rate * 2 * k * ( f ( s ) - y [ i ] ) * ( 1-s ) * f ( s ) * x [ i ] err += 0.5 * ( f ( s ) - y [ i ] ) * * 2 err /= N error.append ( err ) print ( 'Final w : ' , w ) The weights have indeed converged to $ w_0=1 , ~w_1=1 $ : Initial w : [ 1.5915165 0.27594833 ] Final w : [ 1.03561356 0.96695205 ] The training error is decreasing : plt.scatter ( np.arange ( n_epochs ) , error ) plt.grid ( ) plt.xticks ( np.arange ( 0 , n_epochs , step=1 ) ) plt.show ( ) Let 's test it.